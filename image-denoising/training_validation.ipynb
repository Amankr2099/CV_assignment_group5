{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Getting the training dataset","metadata":{}},{"cell_type":"code","source":"!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T06:41:38.786978Z","iopub.execute_input":"2025-03-19T06:41:38.787281Z","iopub.status.idle":"2025-03-19T06:44:06.152589Z","shell.execute_reply.started":"2025-03-19T06:41:38.787256Z","shell.execute_reply":"2025-03-19T06:44:06.151777Z"}},"outputs":[{"name":"stdout","text":"--2025-03-19 06:41:38--  http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\nResolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip [following]\n--2025-03-19 06:41:39--  https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_train_HR.zip\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3530603713 (3.3G) [application/zip]\nSaving to: ‘DIV2K_train_HR.zip’\n\nDIV2K_train_HR.zip  100%[===================>]   3.29G  20.6MB/s    in 2m 26s  \n\n2025-03-19 06:44:06 (23.0 MB/s) - ‘DIV2K_train_HR.zip’ saved [3530603713/3530603713]\n\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import shutil\nimport os\n\nzip_file = \"/kaggle/working/DIV2K_train_HR.zip\"\nextract_folder = \"/kaggle/working/DIV2K_train_HR\"\n\nshutil.unpack_archive(zip_file, extract_folder)\nos.remove(zip_file)\nprint(\"Unzipped and deleted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T06:44:52.443994Z","iopub.execute_input":"2025-03-19T06:44:52.444359Z","iopub.status.idle":"2025-03-19T06:45:08.995983Z","shell.execute_reply.started":"2025-03-19T06:44:52.444288Z","shell.execute_reply":"2025-03-19T06:45:08.995125Z"}},"outputs":[{"name":"stdout","text":"Unzipped and deleted!\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"## Adding Noise to images","metadata":{}},{"cell_type":"code","source":"import os\nimport numpy as np\nimport imageio.v2 as imageio  # Use imageio.v2 to avoid deprecation warnings\nimport torch\nimport zipfile\nimport shutil\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\n# Paths\nHR_DIR = \"/kaggle/working/DIV2K_train_HR/DIV2K_train_HR\"  # High-resolution images\nNOISY_DIR = \"/kaggle/working/DIV2K_train_LR_noisy/DIV2K_train_noisy\"  # Output directory for noisy images\nZIP_FILE_PATH = \"/kaggle/working/noisy_images.zip\"  # Path to the zip file containing processed images\n\n# Ensure output directory exists\nos.makedirs(NOISY_DIR, exist_ok=True)\n\nimport cupy as cp\n\ndef add_noise(image, sigma=50):\n    image = cp.array(image / 255, dtype=cp.float32)\n    noise = cp.random.normal(0, sigma / 255, image.shape)\n    gauss_noise = image + noise\n    return (gauss_noise * 255).get()  # Convert back to NumPy\n\ndef save_image(image, path):\n    \"\"\"\n    Saves an image after clipping and rounding to uint8 format.\n    image: Image as numpy array.\n    path: Save location.\n    \"\"\"\n    image = np.round(np.clip(image, 0, 255)).astype(np.uint8)\n    imageio.imwrite(path, image)\n\ndef crop_image(image, s=8):\n    \"\"\"\n    Crops an image so its width & height are multiples of 's'.\n    \"\"\"\n    h, w, c = image.shape\n    image = image[:h - h % s, :w - w % s, :]\n    return image\n\n# Process all images in the HR directory\nfor img_name in os.listdir(HR_DIR):\n    if img_name.endswith(\".png\"):  # Process only PNG files\n        img_path = os.path.join(HR_DIR, img_name)\n        noisy_img_path = os.path.join(NOISY_DIR, img_name)\n\n        # Read and process the image\n        img = imageio.imread(img_path)\n        img = crop_image(img)  # Ensure size is multiple of 8\n        img_noise = add_noise(img, sigma=50)  # Add noise\n\n        # Save the noisy image\n        save_image(img_noise, noisy_img_path)\n        print(f\"Processed: {img_name}\")\n\n# Create a zip file of all processed images for download\nwith zipfile.ZipFile(ZIP_FILE_PATH, 'w', zipfile.ZIP_DEFLATED) as zipf:\n    for img_name in os.listdir(NOISY_DIR):\n        img_path = os.path.join(NOISY_DIR, img_name)\n        if img_name.endswith(\".png\"):  # Only include PNG files in the zip\n            zipf.write(img_path, os.path.basename(img_path))\n\nprint(\"All images processed and saved in\", NOISY_DIR)\nprint(f\"Processed images have been zipped and can be downloaded from {ZIP_FILE_PATH}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T06:45:36.064549Z","iopub.execute_input":"2025-03-19T06:45:36.064885Z","iopub.status.idle":"2025-03-19T07:01:05.257422Z","shell.execute_reply.started":"2025-03-19T06:45:36.064844Z","shell.execute_reply":"2025-03-19T07:01:05.256404Z"}},"outputs":[{"name":"stdout","text":"Processed: 0051.png\nProcessed: 0604.png\nProcessed: 0348.png\nProcessed: 0570.png\nProcessed: 0599.png\nProcessed: 0212.png\nProcessed: 0183.png\nProcessed: 0422.png\nProcessed: 0745.png\nProcessed: 0713.png\nProcessed: 0322.png\nProcessed: 0517.png\nProcessed: 0555.png\nProcessed: 0195.png\nProcessed: 0330.png\nProcessed: 0143.png\nProcessed: 0055.png\nProcessed: 0475.png\nProcessed: 0146.png\nProcessed: 0544.png\nProcessed: 0054.png\nProcessed: 0050.png\nProcessed: 0302.png\nProcessed: 0462.png\nProcessed: 0627.png\nProcessed: 0217.png\nProcessed: 0722.png\nProcessed: 0632.png\nProcessed: 0010.png\nProcessed: 0749.png\nProcessed: 0300.png\nProcessed: 0172.png\nProcessed: 0080.png\nProcessed: 0290.png\nProcessed: 0374.png\nProcessed: 0420.png\nProcessed: 0379.png\nProcessed: 0596.png\nProcessed: 0036.png\nProcessed: 0734.png\nProcessed: 0431.png\nProcessed: 0356.png\nProcessed: 0759.png\nProcessed: 0378.png\nProcessed: 0320.png\nProcessed: 0726.png\nProcessed: 0792.png\nProcessed: 0245.png\nProcessed: 0285.png\nProcessed: 0097.png\nProcessed: 0491.png\nProcessed: 0335.png\nProcessed: 0015.png\nProcessed: 0772.png\nProcessed: 0099.png\nProcessed: 0603.png\nProcessed: 0688.png\nProcessed: 0464.png\nProcessed: 0546.png\nProcessed: 0631.png\nProcessed: 0357.png\nProcessed: 0259.png\nProcessed: 0776.png\nProcessed: 0364.png\nProcessed: 0591.png\nProcessed: 0171.png\nProcessed: 0478.png\nProcessed: 0638.png\nProcessed: 0573.png\nProcessed: 0065.png\nProcessed: 0595.png\nProcessed: 0715.png\nProcessed: 0225.png\nProcessed: 0480.png\nProcessed: 0687.png\nProcessed: 0095.png\nProcessed: 0616.png\nProcessed: 0022.png\nProcessed: 0343.png\nProcessed: 0504.png\nProcessed: 0529.png\nProcessed: 0380.png\nProcessed: 0489.png\nProcessed: 0437.png\nProcessed: 0535.png\nProcessed: 0474.png\nProcessed: 0690.png\nProcessed: 0218.png\nProcessed: 0338.png\nProcessed: 0223.png\nProcessed: 0659.png\nProcessed: 0006.png\nProcessed: 0576.png\nProcessed: 0758.png\nProcessed: 0074.png\nProcessed: 0458.png\nProcessed: 0166.png\nProcessed: 0789.png\nProcessed: 0461.png\nProcessed: 0077.png\nProcessed: 0375.png\nProcessed: 0103.png\nProcessed: 0601.png\nProcessed: 0520.png\nProcessed: 0724.png\nProcessed: 0052.png\nProcessed: 0155.png\nProcessed: 0532.png\nProcessed: 0202.png\nProcessed: 0685.png\nProcessed: 0344.png\nProcessed: 0620.png\nProcessed: 0008.png\nProcessed: 0173.png\nProcessed: 0434.png\nProcessed: 0706.png\nProcessed: 0206.png\nProcessed: 0342.png\nProcessed: 0252.png\nProcessed: 0390.png\nProcessed: 0447.png\nProcessed: 0469.png\nProcessed: 0764.png\nProcessed: 0388.png\nProcessed: 0312.png\nProcessed: 0039.png\nProcessed: 0011.png\nProcessed: 0334.png\nProcessed: 0207.png\nProcessed: 0179.png\nProcessed: 0488.png\nProcessed: 0147.png\nProcessed: 0104.png\nProcessed: 0126.png\nProcessed: 0198.png\nProcessed: 0760.png\nProcessed: 0100.png\nProcessed: 0295.png\nProcessed: 0238.png\nProcessed: 0228.png\nProcessed: 0031.png\nProcessed: 0476.png\nProcessed: 0424.png\nProcessed: 0318.png\nProcessed: 0477.png\nProcessed: 0405.png\nProcessed: 0648.png\nProcessed: 0621.png\nProcessed: 0293.png\nProcessed: 0040.png\nProcessed: 0016.png\nProcessed: 0636.png\nProcessed: 0721.png\nProcessed: 0019.png\nProcessed: 0003.png\nProcessed: 0429.png\nProcessed: 0249.png\nProcessed: 0045.png\nProcessed: 0521.png\nProcessed: 0709.png\nProcessed: 0384.png\nProcessed: 0459.png\nProcessed: 0332.png\nProcessed: 0689.png\nProcessed: 0438.png\nProcessed: 0387.png\nProcessed: 0186.png\nProcessed: 0044.png\nProcessed: 0626.png\nProcessed: 0675.png\nProcessed: 0519.png\nProcessed: 0594.png\nProcessed: 0669.png\nProcessed: 0742.png\nProcessed: 0500.png\nProcessed: 0432.png\nProcessed: 0313.png\nProcessed: 0516.png\nProcessed: 0416.png\nProcessed: 0700.png\nProcessed: 0076.png\nProcessed: 0634.png\nProcessed: 0460.png\nProcessed: 0414.png\nProcessed: 0278.png\nProcessed: 0554.png\nProcessed: 0466.png\nProcessed: 0679.png\nProcessed: 0231.png\nProcessed: 0163.png\nProcessed: 0127.png\nProcessed: 0404.png\nProcessed: 0727.png\nProcessed: 0436.png\nProcessed: 0263.png\nProcessed: 0780.png\nProcessed: 0017.png\nProcessed: 0021.png\nProcessed: 0137.png\nProcessed: 0360.png\nProcessed: 0656.png\nProcessed: 0671.png\nProcessed: 0667.png\nProcessed: 0543.png\nProcessed: 0124.png\nProcessed: 0101.png\nProcessed: 0723.png\nProcessed: 0354.png\nProcessed: 0190.png\nProcessed: 0737.png\nProcessed: 0732.png\nProcessed: 0539.png\nProcessed: 0694.png\nProcessed: 0492.png\nProcessed: 0226.png\nProcessed: 0799.png\nProcessed: 0209.png\nProcessed: 0454.png\nProcessed: 0056.png\nProcessed: 0435.png\nProcessed: 0270.png\nProcessed: 0294.png\nProcessed: 0030.png\nProcessed: 0096.png\nProcessed: 0152.png\nProcessed: 0309.png\nProcessed: 0425.png\nProcessed: 0184.png\nProcessed: 0607.png\nProcessed: 0588.png\nProcessed: 0433.png\nProcessed: 0698.png\nProcessed: 0112.png\nProcessed: 0139.png\nProcessed: 0248.png\nProcessed: 0013.png\nProcessed: 0292.png\nProcessed: 0079.png\nProcessed: 0363.png\nProcessed: 0611.png\nProcessed: 0457.png\nProcessed: 0508.png\nProcessed: 0369.png\nProcessed: 0619.png\nProcessed: 0329.png\nProcessed: 0398.png\nProcessed: 0291.png\nProcessed: 0164.png\nProcessed: 0514.png\nProcessed: 0412.png\nProcessed: 0605.png\nProcessed: 0132.png\nProcessed: 0150.png\nProcessed: 0129.png\nProcessed: 0506.png\nProcessed: 0582.png\nProcessed: 0445.png\nProcessed: 0283.png\nProcessed: 0121.png\nProcessed: 0047.png\nProcessed: 0281.png\nProcessed: 0028.png\nProcessed: 0418.png\nProcessed: 0229.png\nProcessed: 0149.png\nProcessed: 0541.png\nProcessed: 0793.png\nProcessed: 0257.png\nProcessed: 0386.png\nProcessed: 0767.png\nProcessed: 0130.png\nProcessed: 0024.png\nProcessed: 0786.png\nProcessed: 0310.png\nProcessed: 0730.png\nProcessed: 0057.png\nProcessed: 0540.png\nProcessed: 0314.png\nProcessed: 0449.png\nProcessed: 0641.png\nProcessed: 0305.png\nProcessed: 0455.png\nProcessed: 0256.png\nProcessed: 0355.png\nProcessed: 0153.png\nProcessed: 0512.png\nProcessed: 0666.png\nProcessed: 0609.png\nProcessed: 0351.png\nProcessed: 0784.png\nProcessed: 0136.png\nProcessed: 0073.png\nProcessed: 0444.png\nProcessed: 0623.png\nProcessed: 0795.png\nProcessed: 0253.png\nProcessed: 0328.png\nProcessed: 0523.png\nProcessed: 0023.png\nProcessed: 0567.png\nProcessed: 0798.png\nProcessed: 0580.png\nProcessed: 0618.png\nProcessed: 0372.png\nProcessed: 0787.png\nProcessed: 0038.png\nProcessed: 0224.png\nProcessed: 0359.png\nProcessed: 0162.png\nProcessed: 0066.png\nProcessed: 0681.png\nProcessed: 0482.png\nProcessed: 0347.png\nProcessed: 0001.png\nProcessed: 0439.png\nProcessed: 0624.png\nProcessed: 0086.png\nProcessed: 0590.png\nProcessed: 0004.png\nProcessed: 0151.png\nProcessed: 0144.png\nProcessed: 0770.png\nProcessed: 0114.png\nProcessed: 0107.png\nProcessed: 0768.png\nProcessed: 0189.png\nProcessed: 0781.png\nProcessed: 0373.png\nProcessed: 0227.png\nProcessed: 0556.png\nProcessed: 0664.png\nProcessed: 0562.png\nProcessed: 0720.png\nProcessed: 0705.png\nProcessed: 0463.png\nProcessed: 0407.png\nProcessed: 0649.png\nProcessed: 0235.png\nProcessed: 0670.png\nProcessed: 0509.png\nProcessed: 0299.png\nProcessed: 0487.png\nProcessed: 0085.png\nProcessed: 0497.png\nProcessed: 0067.png\nProcessed: 0702.png\nProcessed: 0116.png\nProcessed: 0654.png\nProcessed: 0396.png\nProcessed: 0395.png\nProcessed: 0498.png\nProcessed: 0371.png\nProcessed: 0258.png\nProcessed: 0352.png\nProcessed: 0337.png\nProcessed: 0672.png\nProcessed: 0068.png\nProcessed: 0693.png\nProcessed: 0625.png\nProcessed: 0584.png\nProcessed: 0325.png\nProcessed: 0063.png\nProcessed: 0282.png\nProcessed: 0538.png\nProcessed: 0264.png\nProcessed: 0194.png\nProcessed: 0581.png\nProcessed: 0232.png\nProcessed: 0280.png\nProcessed: 0053.png\nProcessed: 0367.png\nProcessed: 0545.png\nProcessed: 0062.png\nProcessed: 0502.png\nProcessed: 0362.png\nProcessed: 0222.png\nProcessed: 0237.png\nProcessed: 0142.png\nProcessed: 0704.png\nProcessed: 0602.png\nProcessed: 0221.png\nProcessed: 0401.png\nProcessed: 0443.png\nProcessed: 0196.png\nProcessed: 0336.png\nProcessed: 0537.png\nProcessed: 0327.png\nProcessed: 0273.png\nProcessed: 0678.png\nProcessed: 0668.png\nProcessed: 0547.png\nProcessed: 0123.png\nProcessed: 0719.png\nProcessed: 0729.png\nProcessed: 0211.png\nProcessed: 0204.png\nProcessed: 0701.png\nProcessed: 0389.png\nProcessed: 0485.png\nProcessed: 0007.png\nProcessed: 0216.png\nProcessed: 0697.png\nProcessed: 0274.png\nProcessed: 0635.png\nProcessed: 0686.png\nProcessed: 0640.png\nProcessed: 0622.png\nProcessed: 0756.png\nProcessed: 0415.png\nProcessed: 0288.png\nProcessed: 0366.png\nProcessed: 0034.png\nProcessed: 0575.png\nProcessed: 0043.png\nProcessed: 0089.png\nProcessed: 0788.png\nProcessed: 0453.png\nProcessed: 0561.png\nProcessed: 0026.png\nProcessed: 0494.png\nProcessed: 0633.png\nProcessed: 0020.png\nProcessed: 0158.png\nProcessed: 0692.png\nProcessed: 0094.png\nProcessed: 0098.png\nProcessed: 0557.png\nProcessed: 0032.png\nProcessed: 0652.png\nProcessed: 0563.png\nProcessed: 0138.png\nProcessed: 0134.png\nProcessed: 0446.png\nProcessed: 0658.png\nProcessed: 0542.png\nProcessed: 0279.png\nProcessed: 0027.png\nProcessed: 0046.png\nProcessed: 0490.png\nProcessed: 0210.png\nProcessed: 0324.png\nProcessed: 0178.png\nProcessed: 0579.png\nProcessed: 0642.png\nProcessed: 0505.png\nProcessed: 0663.png\nProcessed: 0311.png\nProcessed: 0525.png\nProcessed: 0778.png\nProcessed: 0160.png\nProcessed: 0703.png\nProcessed: 0087.png\nProcessed: 0157.png\nProcessed: 0239.png\nProcessed: 0661.png\nProcessed: 0743.png\nProcessed: 0534.png\nProcessed: 0177.png\nProcessed: 0752.png\nProcessed: 0660.png\nProcessed: 0339.png\nProcessed: 0733.png\nProcessed: 0551.png\nProcessed: 0170.png\nProcessed: 0154.png\nProcessed: 0587.png\nProcessed: 0250.png\nProcessed: 0254.png\nProcessed: 0481.png\nProcessed: 0769.png\nProcessed: 0286.png\nProcessed: 0287.png\nProcessed: 0048.png\nProcessed: 0766.png\nProcessed: 0680.png\nProcessed: 0180.png\nProcessed: 0560.png\nProcessed: 0191.png\nProcessed: 0653.png\nProcessed: 0326.png\nProcessed: 0271.png\nProcessed: 0140.png\nProcessed: 0064.png\nProcessed: 0739.png\nProcessed: 0241.png\nProcessed: 0070.png\nProcessed: 0234.png\nProcessed: 0483.png\nProcessed: 0220.png\nProcessed: 0495.png\nProcessed: 0061.png\nProcessed: 0377.png\nProcessed: 0268.png\nProcessed: 0346.png\nProcessed: 0266.png\nProcessed: 0518.png\nProcessed: 0441.png\nProcessed: 0496.png\nProcessed: 0323.png\nProcessed: 0410.png\nProcessed: 0569.png\nProcessed: 0790.png\nProcessed: 0782.png\nProcessed: 0381.png\nProcessed: 0501.png\nProcessed: 0391.png\nProcessed: 0527.png\nProcessed: 0383.png\nProcessed: 0797.png\nProcessed: 0201.png\nProcessed: 0484.png\nProcessed: 0800.png\nProcessed: 0577.png\nProcessed: 0350.png\nProcessed: 0275.png\nProcessed: 0553.png\nProcessed: 0716.png\nProcessed: 0712.png\nProcessed: 0399.png\nProcessed: 0289.png\nProcessed: 0185.png\nProcessed: 0393.png\nProcessed: 0564.png\nProcessed: 0765.png\nProcessed: 0361.png\nProcessed: 0592.png\nProcessed: 0473.png\nProcessed: 0301.png\nProcessed: 0175.png\nProcessed: 0088.png\nProcessed: 0448.png\nProcessed: 0578.png\nProcessed: 0629.png\nProcessed: 0385.png\nProcessed: 0009.png\nProcessed: 0574.png\nProcessed: 0133.png\nProcessed: 0315.png\nProcessed: 0467.png\nProcessed: 0078.png\nProcessed: 0296.png\nProcessed: 0059.png\nProcessed: 0208.png\nProcessed: 0785.png\nProcessed: 0029.png\nProcessed: 0260.png\nProcessed: 0565.png\nProcessed: 0167.png\nProcessed: 0233.png\nProcessed: 0536.png\nProcessed: 0426.png\nProcessed: 0081.png\nProcessed: 0370.png\nProcessed: 0403.png\nProcessed: 0470.png\nProcessed: 0628.png\nProcessed: 0333.png\nProcessed: 0522.png\nProcessed: 0111.png\nProcessed: 0135.png\nProcessed: 0037.png\nProcessed: 0308.png\nProcessed: 0203.png\nProcessed: 0735.png\nProcessed: 0106.png\nProcessed: 0306.png\nProcessed: 0486.png\nProcessed: 0754.png\nProcessed: 0113.png\nProcessed: 0699.png\nProcessed: 0791.png\nProcessed: 0321.png\nProcessed: 0409.png\nProcessed: 0472.png\nProcessed: 0606.png\nProcessed: 0741.png\nProcessed: 0376.png\nProcessed: 0182.png\nProcessed: 0747.png\nProcessed: 0637.png\nProcessed: 0513.png\nProcessed: 0471.png\nProcessed: 0109.png\nProcessed: 0421.png\nProcessed: 0423.png\nProcessed: 0683.png\nProcessed: 0193.png\nProcessed: 0341.png\nProcessed: 0710.png\nProcessed: 0005.png\nProcessed: 0452.png\nProcessed: 0187.png\nProcessed: 0156.png\nProcessed: 0083.png\nProcessed: 0657.png\nProcessed: 0695.png\nProcessed: 0707.png\nProcessed: 0025.png\nProcessed: 0230.png\nProcessed: 0122.png\nProcessed: 0304.png\nProcessed: 0033.png\nProcessed: 0748.png\nProcessed: 0307.png\nProcessed: 0297.png\nProcessed: 0284.png\nProcessed: 0465.png\nProcessed: 0428.png\nProcessed: 0600.png\nProcessed: 0613.png\nProcessed: 0402.png\nProcessed: 0615.png\nProcessed: 0665.png\nProcessed: 0572.png\nProcessed: 0515.png\nProcessed: 0176.png\nProcessed: 0773.png\nProcessed: 0071.png\nProcessed: 0430.png\nProcessed: 0358.png\nProcessed: 0608.png\nProcessed: 0197.png\nProcessed: 0440.png\nProcessed: 0644.png\nProcessed: 0345.png\nProcessed: 0442.png\nProcessed: 0597.png\nProcessed: 0548.png\nProcessed: 0614.png\nProcessed: 0397.png\nProcessed: 0331.png\nProcessed: 0676.png\nProcessed: 0243.png\nProcessed: 0718.png\nProcessed: 0261.png\nProcessed: 0317.png\nProcessed: 0255.png\nProcessed: 0651.png\nProcessed: 0340.png\nProcessed: 0617.png\nProcessed: 0082.png\nProcessed: 0753.png\nProcessed: 0353.png\nProcessed: 0586.png\nProcessed: 0779.png\nProcessed: 0411.png\nProcessed: 0192.png\nProcessed: 0365.png\nProcessed: 0303.png\nProcessed: 0200.png\nProcessed: 0247.png\nProcessed: 0012.png\nProcessed: 0131.png\nProcessed: 0277.png\nProcessed: 0090.png\nProcessed: 0382.png\nProcessed: 0427.png\nProcessed: 0677.png\nProcessed: 0552.png\nProcessed: 0128.png\nProcessed: 0242.png\nProcessed: 0738.png\nProcessed: 0468.png\nProcessed: 0161.png\nProcessed: 0740.png\nProcessed: 0708.png\nProcessed: 0246.png\nProcessed: 0084.png\nProcessed: 0717.png\nProcessed: 0408.png\nProcessed: 0507.png\nProcessed: 0049.png\nProcessed: 0725.png\nProcessed: 0014.png\nProcessed: 0524.png\nProcessed: 0583.png\nProcessed: 0118.png\nProcessed: 0269.png\nProcessed: 0684.png\nProcessed: 0630.png\nProcessed: 0267.png\nProcessed: 0673.png\nProcessed: 0493.png\nProcessed: 0169.png\nProcessed: 0549.png\nProcessed: 0755.png\nProcessed: 0102.png\nProcessed: 0531.png\nProcessed: 0771.png\nProcessed: 0763.png\nProcessed: 0120.png\nProcessed: 0058.png\nProcessed: 0298.png\nProcessed: 0125.png\nProcessed: 0394.png\nProcessed: 0550.png\nProcessed: 0319.png\nProcessed: 0568.png\nProcessed: 0728.png\nProcessed: 0174.png\nProcessed: 0714.png\nProcessed: 0105.png\nProcessed: 0662.png\nProcessed: 0262.png\nProcessed: 0018.png\nProcessed: 0526.png\nProcessed: 0598.png\nProcessed: 0240.png\nProcessed: 0528.png\nProcessed: 0119.png\nProcessed: 0219.png\nProcessed: 0368.png\nProcessed: 0148.png\nProcessed: 0757.png\nProcessed: 0265.png\nProcessed: 0499.png\nProcessed: 0736.png\nProcessed: 0746.png\nProcessed: 0400.png\nProcessed: 0213.png\nProcessed: 0456.png\nProcessed: 0181.png\nProcessed: 0392.png\nProcessed: 0647.png\nProcessed: 0145.png\nProcessed: 0244.png\nProcessed: 0559.png\nProcessed: 0199.png\nProcessed: 0406.png\nProcessed: 0115.png\nProcessed: 0533.png\nProcessed: 0417.png\nProcessed: 0117.png\nProcessed: 0272.png\nProcessed: 0696.png\nProcessed: 0646.png\nProcessed: 0750.png\nProcessed: 0168.png\nProcessed: 0035.png\nProcessed: 0214.png\nProcessed: 0655.png\nProcessed: 0691.png\nProcessed: 0762.png\nProcessed: 0643.png\nProcessed: 0682.png\nProcessed: 0450.png\nProcessed: 0777.png\nProcessed: 0731.png\nProcessed: 0571.png\nProcessed: 0510.png\nProcessed: 0589.png\nProcessed: 0503.png\nProcessed: 0110.png\nProcessed: 0761.png\nProcessed: 0236.png\nProcessed: 0165.png\nProcessed: 0413.png\nProcessed: 0585.png\nProcessed: 0002.png\nProcessed: 0511.png\nProcessed: 0419.png\nProcessed: 0650.png\nProcessed: 0276.png\nProcessed: 0060.png\nProcessed: 0612.png\nProcessed: 0796.png\nProcessed: 0093.png\nProcessed: 0069.png\nProcessed: 0349.png\nProcessed: 0674.png\nProcessed: 0610.png\nProcessed: 0042.png\nProcessed: 0566.png\nProcessed: 0041.png\nProcessed: 0251.png\nProcessed: 0774.png\nProcessed: 0108.png\nProcessed: 0205.png\nProcessed: 0092.png\nProcessed: 0751.png\nProcessed: 0639.png\nProcessed: 0744.png\nProcessed: 0783.png\nProcessed: 0558.png\nProcessed: 0775.png\nProcessed: 0072.png\nProcessed: 0645.png\nProcessed: 0711.png\nProcessed: 0530.png\nProcessed: 0451.png\nProcessed: 0593.png\nProcessed: 0215.png\nProcessed: 0794.png\nProcessed: 0188.png\nProcessed: 0091.png\nProcessed: 0479.png\nProcessed: 0159.png\nProcessed: 0316.png\nProcessed: 0141.png\nProcessed: 0075.png\nAll images processed and saved in /kaggle/working/DIV2K_train_LR_noisy/DIV2K_train_noisy\nProcessed images have been zipped and can be downloaded from /kaggle/working/noisy_images.zip\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"## Defining DataLoaders","metadata":{}},{"cell_type":"code","source":"import os\nimport cv2\nimport torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image  # Import PIL\nimport matplotlib.pyplot as plt\n\n# Dataset class\nclass DenoiseDataset(Dataset):\n    def __init__(self, clean_dir, noisy_dir, transform=None):\n        self.clean_images = sorted(os.listdir(clean_dir))\n        self.noisy_images = sorted(os.listdir(noisy_dir))\n        self.clean_dir = clean_dir\n        self.noisy_dir = noisy_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.clean_images)\n\n    def __getitem__(self, idx):\n        clean_path = os.path.join(self.clean_dir, self.clean_images[idx])\n        noisy_path = os.path.join(self.noisy_dir, self.noisy_images[idx])\n\n        # Load images using OpenCV (NumPy arrays)\n        clean_img = cv2.imread(clean_path)\n        noisy_img = cv2.imread(noisy_path)\n\n        # Convert BGR to RGB\n        clean_img = cv2.cvtColor(clean_img, cv2.COLOR_BGR2RGB)\n        noisy_img = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)\n\n        # Convert NumPy array to PIL Image\n        clean_img = Image.fromarray(clean_img)\n        noisy_img = Image.fromarray(noisy_img)\n\n        # Apply transformations\n        if self.transform:\n            clean_img = self.transform(clean_img)\n            noisy_img = self.transform(noisy_img)\n\n        return noisy_img, clean_img\n\n# Data augmentation & transformations\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),\n    transforms.ToTensor()  # Convert to PyTorch tensor\n])\n\n# Define dataset paths\ntrain_clean_dir = \"/kaggle/working/DIV2K_train_HR/DIV2K_train_HR\"\ntrain_noisy_dir = \"/kaggle/working/DIV2K_train_LR_noisy/DIV2K_train_noisy\"\n\n# Create dataset and dataloader\ntrain_dataset = DenoiseDataset(train_clean_dir, train_noisy_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:05:19.264321Z","iopub.execute_input":"2025-03-19T07:05:19.264852Z","iopub.status.idle":"2025-03-19T07:05:21.932216Z","shell.execute_reply.started":"2025-03-19T07:05:19.264825Z","shell.execute_reply":"2025-03-19T07:05:21.931497Z"}},"outputs":[],"execution_count":4},{"cell_type":"markdown","source":"## Defining UNET model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels=3, out_channels=3, features=[64, 128, 256, 512]):\n        super(UNet, self).__init__()\n        \n        # Encoder\n        self.encoders = nn.ModuleList()\n        for feature in features:\n            self.encoders.append(self._conv_block(in_channels, feature))\n            in_channels = feature\n        \n        # Bottleneck\n        self.bottleneck = self._conv_block(features[-1], features[-1] * 2)\n        \n        # Decoder\n        self.decoders = nn.ModuleList()\n        for feature in reversed(features):\n            self.decoders.append(nn.ConvTranspose2d(feature * 2, feature, kernel_size=2, stride=2))\n            self.decoders.append(self._conv_block(feature * 2, feature))\n        \n        # Final Output Layer\n        self.final_conv = nn.Conv2d(features[0], out_channels, kernel_size=1)\n\n    def _conv_block(self, in_channels, out_channels):\n        \"\"\"Double Convolution Block\"\"\"\n        return nn.Sequential(\n            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n            nn.ReLU(inplace=True),\n        )\n\n    def forward(self, x):\n        skip_connections = []\n        for encoder in self.encoders:\n            x = encoder(x)\n            skip_connections.append(x)\n            x = F.max_pool2d(x, kernel_size=2, stride=2)\n        \n        x = self.bottleneck(x)\n\n        skip_connections = skip_connections[::-1]\n        for i in range(0, len(self.decoders), 2):\n            x = self.decoders[i](x)  # Upconvolution\n            skip_connection = skip_connections[i // 2]\n\n            if x.shape != skip_connection.shape:\n                x = F.interpolate(x, size=skip_connection.shape[2:], mode=\"bilinear\", align_corners=True)\n\n            x = torch.cat((skip_connection, x), dim=1)  # Skip connection\n            x = self.decoders[i + 1](x)  # Convolution Block\n\n        return self.final_conv(x)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:05:27.889480Z","iopub.execute_input":"2025-03-19T07:05:27.889897Z","iopub.status.idle":"2025-03-19T07:05:27.899115Z","shell.execute_reply.started":"2025-03-19T07:05:27.889862Z","shell.execute_reply":"2025-03-19T07:05:27.897955Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import torch\nimport torch.optim as optim\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torch.cuda.amp import autocast, GradScaler\n\n# Initialize UNet Model\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = UNet(in_channels=3, out_channels=3).to(device)\n\n# Loss Function & Optimizer\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\nscaler = GradScaler()  # Enable Mixed Precision Training\n\n# Define Transform with Random Cropping\ntrain_transform = transforms.Compose([\n    transforms.RandomCrop(128),  # Crop to 128x128 patches\n    transforms.ToTensor()\n])\n\n# Training Loop\nnum_epochs = 20\naccumulation_steps = 4  # Effective batch size multiplier\n\nfor epoch in range(num_epochs):\n    model.train()\n    epoch_loss = 0\n    optimizer.zero_grad()\n\n    for i, (noisy_imgs, clean_imgs) in enumerate(train_loader):\n        noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)\n\n        with autocast():  # Enable Mixed Precision\n            outputs = model(noisy_imgs)\n            loss = criterion(outputs, clean_imgs) / accumulation_steps  # Scale loss\n\n        scaler.scale(loss).backward()  # Backpropagate scaled loss\n\n        if (i + 1) % accumulation_steps == 0:  # Update weights every 'accumulation_steps' batches\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad()\n\n        epoch_loss += loss.item() * accumulation_steps  # Reverse scaling\n\n    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss/len(train_loader):.6f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T07:05:51.198344Z","iopub.execute_input":"2025-03-19T07:05:51.198684Z","iopub.status.idle":"2025-03-19T08:21:40.534065Z","shell.execute_reply.started":"2025-03-19T07:05:51.198659Z","shell.execute_reply":"2025-03-19T08:21:40.533230Z"}},"outputs":[{"name":"stderr","text":"<ipython-input-6-1c2907ee7dc3>:14: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n  scaler = GradScaler()  # Enable Mixed Precision Training\n<ipython-input-6-1c2907ee7dc3>:34: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n  with autocast():  # Enable Mixed Precision\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/20, Loss: 4.812734\nEpoch 2/20, Loss: 0.091043\nEpoch 3/20, Loss: 0.017900\nEpoch 4/20, Loss: 0.013111\nEpoch 5/20, Loss: 0.007509\nEpoch 6/20, Loss: 0.005184\nEpoch 7/20, Loss: 0.004327\nEpoch 8/20, Loss: 0.003890\nEpoch 9/20, Loss: 0.003581\nEpoch 10/20, Loss: 0.003076\nEpoch 11/20, Loss: 0.002662\nEpoch 12/20, Loss: 0.002370\nEpoch 13/20, Loss: 0.002308\nEpoch 14/20, Loss: 0.002150\nEpoch 15/20, Loss: 0.002039\nEpoch 16/20, Loss: 0.001940\nEpoch 17/20, Loss: 0.001852\nEpoch 18/20, Loss: 0.001804\nEpoch 19/20, Loss: 0.001757\nEpoch 20/20, Loss: 0.001732\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# model = UNet(in_channels=3, out_channels=3).to(device)  # Initialize model\n# model.load_state_dict(torch.load(\"unet_model.pth\"))  # Load weights into the model\n# model.eval() ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from skimage.metrics import peak_signal_noise_ratio as psnr\nfrom skimage.metrics import structural_similarity as ssim\n\n\ndef evaluate_model(model, dataloader, device):\n    model.eval()\n    total_psnr = 0\n    total_ssim = 0\n    num_samples = 0\n\n    with torch.no_grad():\n        for noisy_imgs, clean_imgs in dataloader:\n            noisy_imgs, clean_imgs = noisy_imgs.to(device), clean_imgs.to(device)\n            denoised_imgs = model(noisy_imgs)\n\n            for i in range(noisy_imgs.shape[0]):\n                clean_np = clean_imgs[i].cpu().numpy().transpose(1, 2, 0)  # (H, W, C)\n                denoised_np = denoised_imgs[i].cpu().numpy().transpose(1, 2, 0)\n\n                clean_np = np.clip(clean_np, 0, 1)\n                denoised_np = np.clip(denoised_np, 0, 1)\n\n                # Compute PSNR\n                img_psnr = psnr(clean_np, denoised_np, data_range=1.0)\n\n                # Compute SSIM with win_size fix\n                try:\n                    img_ssim = ssim(clean_np, denoised_np, data_range=1.0, win_size=3, channel_axis=-1)\n                except ValueError as e:\n                    print(f\"Skipping SSIM for small image: {clean_np.shape} - {e}\")\n                    img_ssim = 0  # Assign 0 if SSIM cannot be computed\n\n                total_psnr += img_psnr\n                total_ssim += img_ssim\n                num_samples += 1\n\n    avg_psnr = total_psnr / num_samples\n    avg_ssim = total_ssim / num_samples\n    print(f\"Validation PSNR: {avg_psnr:.2f} dB, SSIM: {avg_ssim:.4f}\")\n    return avg_psnr, avg_ssim","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:23:36.663408Z","iopub.execute_input":"2025-03-19T08:23:36.663756Z","iopub.status.idle":"2025-03-19T08:23:37.483546Z","shell.execute_reply.started":"2025-03-19T08:23:36.663726Z","shell.execute_reply":"2025-03-19T08:23:37.482895Z"}},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":"## Getting Validation Dataset","metadata":{}},{"cell_type":"code","source":"!gdown --fuzzy --id 1iYurwSVBUxoN6fQwUGP-UbZkTZkippGx","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:22:43.573631Z","iopub.execute_input":"2025-03-19T08:22:43.573962Z","iopub.status.idle":"2025-03-19T08:22:55.839663Z","shell.execute_reply.started":"2025-03-19T08:22:43.573936Z","shell.execute_reply":"2025-03-19T08:22:55.838587Z"}},"outputs":[{"name":"stdout","text":"/usr/local/lib/python3.10/dist-packages/gdown/__main__.py:140: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n  warnings.warn(\nDownloading...\nFrom (original): https://drive.google.com/uc?id=1iYurwSVBUxoN6fQwUGP-UbZkTZkippGx\nFrom (redirected): https://drive.google.com/uc?id=1iYurwSVBUxoN6fQwUGP-UbZkTZkippGx&confirm=t&uuid=73c0aa0a-c04b-44c2-80c4-5f5fbbf16485\nTo: /kaggle/working/noise.zip\n100%|█████████████████████████████████████████| 811M/811M [00:07<00:00, 103MB/s]\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"!wget -c http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:23:07.510927Z","iopub.execute_input":"2025-03-19T08:23:07.511253Z","iopub.status.idle":"2025-03-19T08:23:25.643230Z","shell.execute_reply.started":"2025-03-19T08:23:07.511224Z","shell.execute_reply":"2025-03-19T08:23:25.642112Z"}},"outputs":[{"name":"stdout","text":"--2025-03-19 08:23:07--  http://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\nResolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.178, 2001:67c:10ec:36c2::178\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:80... connected.\nHTTP request sent, awaiting response... 302 Found\nLocation: https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip [following]\n--2025-03-19 08:23:08--  https://data.vision.ee.ethz.ch/cvl/DIV2K/DIV2K_valid_HR.zip\nConnecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.178|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 448993893 (428M) [application/zip]\nSaving to: ‘DIV2K_valid_HR.zip’\n\nDIV2K_valid_HR.zip  100%[===================>] 428.19M  26.3MB/s    in 17s     \n\n2025-03-19 08:23:25 (25.3 MB/s) - ‘DIV2K_valid_HR.zip’ saved [448993893/448993893]\n\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"import shutil\nimport os\n\nzip_file = \"//kaggle/working/noise.zip\"\nextract_folder = \"/kaggle/working/noise\"\n\nshutil.unpack_archive(zip_file, extract_folder)\nos.remove(zip_file)\nprint(\"Unzipped and deleted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:23:49.582263Z","iopub.execute_input":"2025-03-19T08:23:49.582853Z","iopub.status.idle":"2025-03-19T08:23:51.588249Z","shell.execute_reply.started":"2025-03-19T08:23:49.582818Z","shell.execute_reply":"2025-03-19T08:23:51.586993Z"}},"outputs":[{"name":"stdout","text":"Unzipped and deleted!\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"import shutil\nimport os\n\nzip_file = \"/kaggle/working/DIV2K_valid_HR.zip\"\nextract_folder = \"/kaggle/working/DIV2K_valid_HR\"\n\nshutil.unpack_archive(zip_file, extract_folder)\nos.remove(zip_file)\nprint(\"Unzipped and deleted!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:23:55.215599Z","iopub.execute_input":"2025-03-19T08:23:55.215901Z","iopub.status.idle":"2025-03-19T08:23:57.495067Z","shell.execute_reply.started":"2025-03-19T08:23:55.215877Z","shell.execute_reply":"2025-03-19T08:23:57.494044Z"}},"outputs":[{"name":"stdout","text":"Unzipped and deleted!\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"class Val(Dataset):\n    def __init__(self, clean_dir, noisy_dir, transform=None):\n        self.clean_images = sorted(os.listdir(clean_dir))\n        self.noisy_images = sorted(os.listdir(noisy_dir))\n        self.clean_dir = clean_dir\n        self.noisy_dir = noisy_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.clean_images)\n\n    def __getitem__(self, idx):\n        clean_path = os.path.join(self.clean_dir, self.clean_images[idx])\n        noisy_path = os.path.join(self.noisy_dir, self.noisy_images[idx])\n\n        # Load images using OpenCV (NumPy arrays)\n        clean_img = cv2.imread(clean_path)\n        noisy_img = cv2.imread(noisy_path)\n\n        # Convert BGR to RGB\n        clean_img = cv2.cvtColor(clean_img, cv2.COLOR_BGR2RGB)\n        noisy_img = cv2.cvtColor(noisy_img, cv2.COLOR_BGR2RGB)\n\n        # Convert NumPy array to PIL Image\n        clean_img = Image.fromarray(clean_img)\n        noisy_img = Image.fromarray(noisy_img)\n\n        # Apply transformations\n        if self.transform:\n            clean_img = self.transform(clean_img)\n            noisy_img = self.transform(noisy_img)\n\n        return noisy_img, clean_img\n\n# Data augmentation & transformations\ntransform = transforms.Compose([\n    transforms.Resize((512, 512)),  # Resize all images to 512x512\n    transforms.ToTensor()  # Convert to PyTorch tensor\n])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:23:59.392519Z","iopub.execute_input":"2025-03-19T08:23:59.392808Z","iopub.status.idle":"2025-03-19T08:23:59.399627Z","shell.execute_reply.started":"2025-03-19T08:23:59.392787Z","shell.execute_reply":"2025-03-19T08:23:59.398708Z"}},"outputs":[],"execution_count":13},{"cell_type":"markdown","source":"## Evaluation on Validation Dataset","metadata":{}},{"cell_type":"code","source":"# Define validation dataset paths\nval_clean_dir = \"/kaggle/working/DIV2K_valid_HR/DIV2K_valid_HR\"\nval_noisy_dir = \"/kaggle/working/noise/noise\"\n\n# Create validation dataset and dataloader\nval_dataset = Val(val_clean_dir, val_noisy_dir, transform=transform)\nval_loader = DataLoader(val_dataset, batch_size=8, shuffle=False)\n\n# Evaluate Model\nevaluate_model(model, val_loader, device)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:24:20.552959Z","iopub.execute_input":"2025-03-19T08:24:20.553290Z","iopub.status.idle":"2025-03-19T08:24:54.254177Z","shell.execute_reply.started":"2025-03-19T08:24:20.553265Z","shell.execute_reply":"2025-03-19T08:24:54.253313Z"}},"outputs":[{"name":"stdout","text":"Validation PSNR: 28.23 dB, SSIM: 0.8022\n","output_type":"stream"},{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"(28.227648598152303, 0.8021559178829193)"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"\n# Save the trained model\nmodel_save_path = \"unet_model.pth\"\ntorch.save(model.state_dict(), model_save_path)\nprint(f\"Model saved to {model_save_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-19T08:30:45.850628Z","iopub.execute_input":"2025-03-19T08:30:45.850959Z","iopub.status.idle":"2025-03-19T08:30:46.025076Z","shell.execute_reply.started":"2025-03-19T08:30:45.850931Z","shell.execute_reply":"2025-03-19T08:30:46.024258Z"}},"outputs":[{"name":"stdout","text":"Model saved to unet_model.pth\n","output_type":"stream"}],"execution_count":17}]}